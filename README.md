# ðŸš— CARLOS  
### Vision-Based Autonomous Mobile Platform  

Real-time lane detection and traffic sign recognition system running on Raspberry Pi 4.

---

## ðŸ“Œ About the Project

CARLOS (Cognitive Autonomous Road-Learning Operating System) is a small-scale autonomous mobile platform developed as an engineering thesis project.

The system is designed to:

- Follow a marked lane using a monocular camera  
- Detect and classify selected traffic signs  
- Adapt steering in real time using differential drive control  
- Operate autonomously without human intervention  

This repository contains the **complete software stack** running on the robot.

---

## ðŸ§  System Overview

CARLOS integrates:

- Computer Vision (OpenCV-based)
- Real-time control logic
- PWM motor control
- Raspberry Pi hardware interface
- Differential drive kinematics

### High-Level Pipeline

Camera Frame
â†“
Lane Detection â†’ Steering Error
Sign Detection â†’ Event Info
â†“
Drive Control (P-controller + fallback)
â†“
Motor PWM Output


---

## ðŸ›£ Lane Detection Module

Implemented using classical computer vision techniques (no machine learning):

- BGR â†’ HSV thresholding
- Morphological filtering
- Perspective transform (birdâ€™s-eye view)
- Histogram-based lane localization
- Polynomial fitting
- Lateral & heading error estimation

**Output:**
- Lane center offset
- Lane validity flag
- Steering correction input

---

## ðŸš¸ Traffic Sign Detection Module

Multi-stage detection pipeline:

1. Edge detection (Canny)
2. Contour extraction
3. Shape classification (triangle / circle / octagon)
4. HSV dominant color filtering
5. ORB feature matching against internal sign database

**Supported classes:**
- STOP
- Speed limits
- Warning signs
- Mandatory signs

---

## ðŸŽ› Control Logic

The `Drive_Control` module implements:

- Proportional steering controller
- Differential speed mapping:

  ```python
  left  = base_speed - steering
  right = base_speed + steering

- Steering saturation limits
- Emergency fallback mode when lane detection becomes unreliable

## âš™ï¸ Hardware Platform

The software runs on a fully integrated autonomous mobile platform consisting of:

- **Raspberry Pi 4 Model B** â€“ main computing unit  
- **CSI 5MP camera with 180Â° wide-angle lens** â€“ monocular vision system  
- **Dual N20 DC gear motors (75:1)** â€“ differential drive propulsion  
- **Cytron MDD3A motor driver** â€“ dual-channel H-bridge control  
- **Differential drive configuration** â€“ independent left/right wheel control  
- **Separated power architecture** â€“ independent supplies for logic and motors  

The hardware architecture was designed to support real-time vision processing while maintaining stable motor control under embedded constraints.

---

## ðŸ›  Technologies Used

- **Python**
- **OpenCV**
- **NumPy**
- **GPIO + PWM motor control**
- **Object-Oriented Programming (modular architecture)**
- **Real-time image processing on embedded Linux (Raspberry Pi OS)**

The system avoids machine learning by design and relies entirely on deterministic computer vision and control algorithms.

---

## ðŸ”¬ Engineering Challenges Addressed

During development, several real-world engineering challenges were identified and addressed:

- Wide-angle lens distortion affecting contour-based sign detection  
- Sensitivity of HSV segmentation to lighting conditions  
- Real-time processing constraints on Raspberry Pi hardware  
- Steering instability under partial lane loss  
- Separation of perception, decision, and execution layers  
- Power supply noise isolation between motors and logic system  

The project focuses not only on functionality but also on robustness under practical embedded conditions.

---

## ðŸš€ Current Status

- âœ” Fully integrated hardware + software system  
- âœ” Real-time lane following  
- âœ” Real-time traffic sign recognition  
- âœ” Adaptive steering control  
- âœ” Laboratory validation completed  

### Ongoing Improvements

- Crossroad detection refinement  
- Improved fallback robustness  
- Lens distortion correction  
- Sign-based speed adaptation  
- Performance optimization for higher frame rate stability  

---

## â–¶ How to Run

1. Clone the repository:

git clone https://github.com/Tomasz-Marek/Autonomous_car.git
cd CARLOS


2. Create and activate a virtual environment:

python -m venv .venv
source .venv/bin/activate # Linux / macOS
..venv\Scripts\activate # Windows


3. Install dependencies:

pip install -r requirements.txt

4. Run the main control loop:

python Python_codes/Main.py


Ensure that the Raspberry Pi camera and motor driver are properly connected before execution.

---

## ðŸŽ“ Background

Developed as part of an engineering thesis focused on:

> Vision-based autonomous navigation and adaptive control in small-scale mobile platforms.

The project integrates mechanical design, embedded electronics, real-time computer vision, and control theory into a cohesive autonomous system.

---

## ðŸ“„ License

MIT License
